---
title: "R Notebook"
output: html_notebook
---


```{r}
ph.data <- read.csv("/Users/JeffAtLaptop/Dropbox/School/DATA624-PredictiveAnalytics/Project2/StudentData.csv",header = TRUE, stringsAsFactors = FALSE)
summary(ph.data)
```

```{r,warning=FALSE}
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(mice)))
suppressWarnings(suppressMessages(library(fBasics)))

# setup the dataset with just jeffs variables
jeffsList <- c("PH","Filler.Level", "Filler.Speed","Temperature","Usage.cont","Carb.Flow","Density","MFR","Balling")

jeffsVars <- data.frame(ph.data[,jeffsList])
summary(jeffsVars)

# run the basic stats on the variables
# Let's start by exploring the type of each variable
types <- sapply(1:length(jeffsVars),function(x) typeof(jeffsVars[,x]))
types.df <- data.frame(VAR=names(jeffsVars),TYPE=types)
kable(types.df)

# Show a statistical summary of the data
kable(summary(jeffsVars[,1:5]))
kable(summary(jeffsVars[,6:9]))

# based on the summary, some of the PH values are 0.0, these rows should be removed since
# we cannot calculate for a 0.0 PH
jeffsVars <- jeffsVars[jeffsVars$PH>0.0,]

# now we'll check how many variables have values of zero
# show the frequency of zeros in the data for each variable
apply(jeffsVars,2,function(x){sum(abs(x-0.0)<=1e-6)})
# based on these counts, we'll replace zeros with NAs for the following variables
index <- which(jeffsVars$Filler.Level <= 0.0)
is.na(jeffsVars$Filler.Level) <- index
index <- which(jeffsVars$Filler.Speed <= 0.0)
is.na(jeffsVars$Filler.Speed) <- index
index <- which(jeffsVars$Temperature <= 0.0)
is.na(jeffsVars$Temperature) <- index
index <- which(jeffsVars$Usage.cont <= 0.0)
is.na(jeffsVars$Usage.cont) <- index
index <- which(jeffsVars$Carb.Flow <= 0.0)
is.na(jeffsVars$Carb.Flow) <- index
index <- which(jeffsVars$MFR<= 0.0)
is.na(jeffsVars$MFR) <- index
# now we'll impute the NA values which represent missing values 
#uses Predictive Mean Matching. 
jeffsVars.imp <- complete(mice(jeffsVars, m = 3, print=F))
#jeffsVars.imp <- complete(jeffsVars.tmp,1)
# check that the NAs have all been resolved with imputed data
any(is.na(jeffsVars.imp))
# generate the basic stats for all variables, including the imputed values
kable(basicStats(jeffsVars.imp[,1:5]))
kable(basicStats(jeffsVars.imp[,6:9]))
```
Now we'll proceed with our analysis of the predictor variables.

```{r,warning=FALSE}
# set up the data combinations needed for the analysis
jeffsVars.pred <- jeffsVars.imp[,-c(1)]
head(jeffsVars.pred)

# required data sets include a train and test set
# form the training and test data partitions
n = nrow(jeffsVars.imp)
index <- sample(1:n, size = round(.80*n), replace = FALSE)
jeffsVars.train <- jeffsVars.imp[index,]
jeffsVars.test <- jeffsVars.imp[-index,]
jeffsVars.pred.train <- jeffsVars.pred[index,]
jeffsVars.pred.test <- jeffsVars.pred[-index,]

```

```{r,echo=FALSE,fig.width = 8, fig.height = 3}
# now we'll analyze each variable to get a sense of the distribution of the data
# Look at the boxplots of the numeric variables
f <- colnames(jeffsVars.pred)  # establish the data categories to be studied
par(mfrow=c(1,3))

for (i in 1:length(f)){
  boxplot(jeffsVars.pred[,i],main = f[i])
}

# we also need to look at the histograms for the numeric variables
for (i in 1:length(f)){
  m <- mean(jeffsVars.pred[,i])
  s <- sd(jeffsVars.pred[,i])  
  hist(jeffsVars.pred[,i],freq=FALSE,main = f[i],xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
}

# let's also look at a quick plot of the data for each variable
for (i in 1:length(f)){
  plot(jeffsVars.pred[,i],main = f[i],xlab="",ylab="")
}
```
Based on a quick analysis of the basic statistics and plots, all of the predictor variables except Filler.Level will require a transformation to address issues with the variable's data distribution.  
  
The Filler.Level variable distribution is fairly normal with few outliers so we will not perform any transformations and, instead, perform an analysis of how the variable interacts with the response variable in various regression models.  

```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Filler.Level, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Filler.Level, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Filler.Level, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Filler.Level, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Filler.Level, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```



Next, we'll explore the effect of Box Cox transformations on the predictor variables with skewed or non-normal distributions.  
  
We'll start with Filler.Speed first.  
```{r,warning=FALSE}
# perform the Box Cox transformation and then look at the distribution
lambda <- BoxCox.lambda(jeffsVars.pred$Filler.Speed)
trans <- BoxCox(jeffsVars.pred$Filler.Speed,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Filler.Speed",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Filler.Speed.Trans=trans)
```

```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Filler.Speed.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Filler.Speed.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Filler.Speed.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Filler.Speed.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Filler.Speed.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  
Next, we'll work with the Temperature variable.  
We'll start with Filler.Speed first.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$Temperature)
trans <- BoxCox(jeffsVars.pred$Temperature,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Temperature",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Temperature.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Temperature.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Temperature.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Temperature.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Temperature.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Temperature.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  
Next, we'll work with the Usage.cont variable.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$Usage.cont)
trans <- BoxCox(jeffsVars.pred$Usage.cont,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Usage.cont",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Usage.cont.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Usage.cont.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Usage.cont.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Usage.cont.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Usage.cont.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Usage.cont.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  
Next, we'll work with the Carb.Flow variable.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$Carb.Flow)
trans <- BoxCox(jeffsVars.pred$Carb.Flow,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Carb.Flow",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Carb.Flow.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Carb.Flow.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Carb.Flow.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Carb.Flow.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Carb.Flow.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Carb.Flow.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```

Next, we'll work with the Density variable.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$Density)
trans <- BoxCox(jeffsVars.pred$Density,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Density",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Density.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Density.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Density.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Density.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Density.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Density.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  
Next, we'll work with the MFR variable.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$MFR)
trans <- BoxCox(jeffsVars.pred$MFR,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "MFR",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,MFR.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~MFR.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~MFR.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~MFR.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~MFR.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~MFR.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  
Next, we'll work with the Bslling variable.  
```{r,warning=FALSE}
# look at a couple of the variables first
lambda <- BoxCox.lambda(jeffsVars.pred$Balling)
trans <- BoxCox(jeffsVars.pred$Balling,lambda)
  m <- mean(trans)
  s <- sd(trans)  
  hist(trans,freq=FALSE,main = "Balling",xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
  plot(trans)
jeffsVars.imp <- cbind(jeffsVars.imp,Balling.Trans=trans)
```
  
```{r,warning=FALSE}
# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(PH~Balling.Trans, data=jeffsVars.imp, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(PH~Balling.Trans, data=jeffsVars.imp, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(624)
#fit.glmnet <- train(PH~Filler.Speed, data=jeffsVars.imp, method="glmnet", metric=metric,
#                    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(624)
fit.svm <- train(PH~Balling.Trans, data=jeffsVars.imp, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(PH~Balling.Trans, data=jeffsVars.imp, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(PH~Balling.Trans, data=jeffsVars.imp, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Compare algorithms
#feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
#                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)

```
  


