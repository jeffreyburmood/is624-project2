---
title: "Data624_Project2"
author: "Thomas Detzley, Arindam Barman, Jeffrey Burmood, Kumudini Bhave"
date: "November 18, 2017"
always_allow_html: yes
output: 
     html_document:
          fontsize: 35pt
          highlight: pygments
          theme: cerulean
          toc: yes
     pdf_document:
          number_sections: yes
          toc: yes
          toc_depth: 3
     word_document: default
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadlib, include=FALSE, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}



# Libraries

suppressMessages(library(easypackages))
suppressWarnings(suppressMessages(libraries("RCurl", "tidyr","dplyr","DT","knit", "ggplot2","Amelia","corrplot","mi","mice","DMwR","fBasics","forecast","tidyverse", "nnet", "kernlab", "caret", "car", "earth", "pander", "randomForest", "mlbench","ModelMetrics", "lars", "MASS", "stats", "pls", "psych", "corrplot", "Hmisc", "mi", "betareg", "mice", "rpart", "party", "partykit", "gbm", "ipred", "VIM", "elasticnet")))
library(ggmosaic)
```



## Predicting PH For Beverages


### Reading the data


```{r loaddata, warning=FALSE,message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

# Retrieving the csv file of dataset from github for reproducibility

# NEED TO PUT THS IS JEFFREYS GITHUB UNDER DATA AND GIVE A RAW LINK TO THAT
studentdata.url <- "https://raw.githubusercontent.com/jeffreyburmood/is624-project2/master/code/data/StudentData.csv"

# the dataset contains blanks which we have marked as NA values while importing
studentdatadf <- read.csv(studentdata.url, header=T,sep=",", strip.white=T, na.strings=c("","NA"))

#REMOVED THIS LINE AS NEW FILE DOESNT GENERATE THiS EXRTA COLUMN,
#studentdatadf <- studentdatadf[,-1] # Since a column X was introduced when converint into csv
     
# convert to data.frame
#View((studentdatadf)) # to be commented out later

# No Of observations and possible predictor variabless
nrow(studentdatadf)
ncol(studentdatadf)


# Studying the Validation Dataset for which PH values are to be predicted

studentdataPREDICT.url <-"https://raw.githubusercontent.com/jeffreyburmood/is624-project2/master/code/data/StudentEvaluation-%20TO%20PREDICT.csv"


# the dataset contains blanks which we have marked as NA values while importing

studentdataPREDICTdf <- read.csv(studentdataPREDICT.url, header=T,sep=",", strip.white=T, na.strings=c("","NA"))

     
# convert to data.frame
#View((studentdataPREDICTdf)) # to be commented out later

# No Of observations and possible predictor variabless
nrow(studentdataPREDICTdf)
ncol(studentdataPREDICTdf)

```


### Data Exploration

**Compare NA counts Study Data And Validation Data**

```{r compare, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

# count NA
studentdatadf_missing <- data.frame(cbind(colSums(is.na(studentdatadf))))
colnames(studentdatadf_missing) <- "Study Data Missing"

studentdataPREDICTdf_missing <- data.frame(cbind(colSums(is.na(studentdataPREDICTdf))))
colnames(studentdataPREDICTdf_missing) <- "Validation Data Missing"

# compare NA in train and validation datasets
compare_missing <- as.data.frame(cbind(studentdatadf_missing, studentdataPREDICTdf_missing))
pander(compare_missing, caption= "Missing Values Comparison For Training And Validation Data")



# compute percent missing
pMiss <- function(x){sum(is.na(x))/length(x)*100}
studentdatadf_pct <- apply(studentdatadf, 2, pMiss)
studentdatadf_pct
studentdataPREDICTdf_pct <- apply(studentdataPREDICTdf, 2, pMiss)

# compare percent missing 
compare_pct_missing <- round(as.data.frame(cbind(studentdatadf_pct, studentdataPREDICTdf_pct)), 2)
colnames(compare_pct_missing) <- c("Study Data Missing %","Validation Data Missing %")
pander(compare_pct_missing, caption= "Missing % Comparison For Training And Validation Data")


```



**Response Variable**

The **PH** is the response variable in the dataset. 

**Histogram of response varaible : PH**
```{r dataex, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

ggplot(studentdatadf, aes(x=PH)) + geom_histogram() + ggtitle("Histogram Response variable : PH")

```

#### Summary of Predictors
We will now look at the summary of all the continuous predictor variables.

```{r datasumm, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}


# Displaying Summary Results Of Possible Constinuous Predictors
summsubdata <- rbind(
#summary(studentdatadf$Brand.Code),
summary(studentdatadf$Carb.Volume),
summary(studentdatadf$Fill.Ounces),
summary(studentdatadf$PC.Volume),
summary(studentdatadf$Carb.Pressure),
summary(studentdatadf$Carb.Temp),
summary(studentdatadf$PSC),
summary(studentdatadf$PSC.Fill),
summary(studentdatadf$PSC.CO2),
summary(studentdatadf$Mnf.Flow),
summary(studentdatadf$Carb.Pressure1),
summary(studentdatadf$Fill.Pressure),
summary(studentdatadf$Hyd.Pressure1),
summary(studentdatadf$Hyd.Pressure2),
summary(studentdatadf$Hyd.Pressure3),
summary(studentdatadf$Hyd.Pressure4),
summary(studentdatadf$Filler.Level),
summary(studentdatadf$Filler.Speed),
summary(studentdatadf$Temperature),
summary(studentdatadf$Usage.cont),
summary(studentdatadf$Carb.Flow),
summary(studentdatadf$Density),
summary(studentdatadf$MFR),
summary(studentdatadf$Balling),
summary(studentdatadf$Pressure.Vacuum),
summary(studentdatadf$Oxygen.Filler),
summary(studentdatadf$Bowl.Setpoint),
summary(studentdatadf$Air.Pressurer),
summary(studentdatadf$Alch.Rel),
summary(studentdatadf$Carb.Rel),
summary(studentdatadf$Balling.Lvl)
)
#"Brand.Code",
rownames(summsubdata) <- c("Carb.Volume","Fill.Ounces","PC.Volume","Carb.Pressure","Carb.Temp","PSC","PSC.Fill","PSC.CO2","Mnf.Flow","Carb.Pressure1","Fill.Pressure","Hyd.Pressure1","Hyd.Pressure2","Hyd.Pressure3","Hyd.Pressure4","Filler.Level","Filler.Speed","Temperature","Usage.cont","Carb.Flow","Density","MFR","Balling","Pressure.Vacuum","Oxygen.Filler","Bowl.Setpoint","Air.Pressurer","Alch.Rel","Carb.Rel","Balling.Lvl")

pander(summsubdata)


```



Studying the data we find : (THIS SECTION IS TO BE ADDED TO/ EDITED)

In Brand variable there are 120 records having no Brand levels and assigned to NA.

Also following variables having NA values  Carb.Volume =10 , Fill.Ounces=38 , PC.Volume=39, Carb.Pressure =27, Carb.Temp= 26, Psc=33, psc.Fill=23, PH=4 (response variable). 


Analysis of missing values Doing summary off missing values by combinations of variables. There are only 2 cases where more than 4 variables are missing for a row.



The PSC CO2 values are all near zero values, so a '0' value is very much possible and of some meaning .So a 0 valuee for this variable need not be considered as an NA.

The Mnf.Flow values are in negative and mostly around -100, but we do see a minimum value of 0 which could be actually an NA / not recorded value and not a real value
Similarly Carbon Pressure1, Fill Pressure has most of the value above 100 or in 40 - 60 range respectively and a few possibly NA values which are recorded as 0 value.

Hyd Pressure 1,Hyd Pressure 2, Hyd Pressure 3 are similar in range and have some NA values with 0 recorded . 

Hyd Pressure 4 range is a bit higher more aournd 80 to 120; a zero value here is an NA value


NOTE: THIS NEEDS TO BE ADDED :more info on other variables


#### Histograms Of Predictors
Lets plot the variables.



```{r datahist, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}
# Plots for all varaibles


multi.hist(studentdatadf[2:8])
multi.hist(studentdatadf[9:16])
multi.hist(studentdatadf[17:24])
multi.hist(studentdatadf[25:33])


```


#### Handling Missing Values

We  see missing values as well as lot of zero values in many predictor variable.However it may very well be so that all the '0' (zero) values that we see could be possibly 'NA'. We will keep these zero values as real values and go ahead with process by just imputing the NA .
We will further impute the data. Also, We would need to look at each of the variables and study them to decide if the zero values  made sense or if it needs to be considered as a wrong recording of data.

We will use the **MICE** package for imputing the NA values.

```{r missval,echo=FALSE, results="hide", warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}


pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(studentdatadf,2, pMiss)
apply(studentdatadf,1,pMiss)
# inspect missing values
md.pattern(studentdatadf)
```

```{r missvalplot, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}



# plot missing values
mice_plot <- aggr(studentdatadf, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(studentdatadf), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))


# impute with Predictive mean matching method

studentdataimp <- mice(studentdatadf, m=5, maxit = 50, method = 'pmm', seed = 500, printFlag = F)

summary(studentdataimp)


# checking for any NAs
anyNA(studentdataimp)

# Lets go ahead with the dataset#2 from the 5 datasets
studentdatapmm <- mice::complete(studentdataimp,2)



```

Let's see the plots after imputation


```{r dataplot, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

# density plot for each var
#par(mfrow=c(16,2))
for(i in c(2:33)) {
ggplot(studentdatapmm, aes(x= studentdatapmm[,i])) + geom_density()
}



# Separating the Predictor set for plotting
predictorset <- studentdatapmm[,c(1,2:25,27:33)]
dim(predictorset)
#View(studentdatadf)
#class(predictorset [,11])

# Bar Plot For Predictor Brand.Code
counts <- table(studentdatapmm$Brand.Code,studentdatapmm$PH )
barplot(counts, main="Brand Code Distribution", 
  	xlab="PH",ylab="Count", col=c("light blue","blue", "dark blue", "turquoise"),
 	legend = rownames(counts))




# Box Plot For Predictors
colnamepred <- colnames(predictorset)
#par(mfrow=c(16,2))
for (i in 2:length(predictorset)){
  boxplot(predictorset[,i],main = paste("BoxPlot : ", colnamepred[i]), col = "light blue")
}


# Plots With Response Variable : PH

for(i in c(2:25, 27:33)) {
  plot( y=studentdatapmm$PH, x=studentdatapmm[,i], xlab=colnames(studentdatapmm)[i], ylab="Response : PH", col="lightblue")
  abline(lsfit(studentdatapmm[,i], studentdatapmm$PH), col="red", lwd=2)
}




```


NOTE: THIS NEEDS TO BE ADDED :

Description or analysis of plots for most variable

which variables to keep for models 

which variables to get rid off for the models



**Correlation Matrix.**

Lets plot the correlation plot of the imputed dataset

```{r corr, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

# Correlation Plot for continuous predictors
correlations <- cor(studentdatapmm[,2:33],use="pairwise.complete.obs")
corrplot::corrplot(correlations, type="lower", tl.cex = 0.7, mar=c(0,4,0,4),c1.cex=2.5)



```


**Transformation**  NOTE: THIS NEEDS TO BE ADDED

Based on a quick analysis of the basic statistics and plots, all of the predictor variables except Filler.Level will require a transformation to address issues with the variable's data distribution.  

The Filler.Level variable distribution is fairly normal with few outliers so we will not perform any transformations and, instead, perform an analysis of how the variable interacts with the response variable in various regression models.  



**Forming Training And Test Datasets**

```{r traintest, warning=FALSE, message=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=150)}

set.seed(21)
randomobs <- sample(seq_len(nrow(studentdatapmm)), size = floor(0.8 * nrow(studentdatapmm)))

trainnew <- studentdatapmm[randomobs,]
testnew <- studentdatapmm[-randomobs,]


```


NOTE : For All Plots, in next version, I will tweak the figure width parameter to fit in a row and use par(mfrow ...)


**Model**

# Model Training and Tuning 
-------------------------  
- List models we tried  
- Discuss parameters and resampling  
- Comparative charts of R^2, RMSE, MAE  
- Concludes by explaining why we chose two top models  
  
Now that we have performed an exploration of the data set and analyzed the behavior and distirbutions of the variables within the data set, we move on to an exploration of various regression models and their performance with regards to generating accurate predictions for the data set response variable, specifically, the PH variable.  
  
The candidate regression models will be evaluated using a training data set which represents 80% of the data values from the historical data in the data set. The remaining 20% of the data values represent a test data set used to evaluate the accuracy of the predictions from our candidate models.  
  
We will evaluate three categories of regression models:  
* Linear Regression Models  
* Non-linear Regression Models  
* Tree-based Rgression Models  

For each model we will pre-process the data by centering and scaling the data to ensure uniform treatment during model tuning. Next, each model will be tuned, or optimized, using algorithms appropriate to the model. A tuned model is a model where the applicable configuration parameters have been mathematically evaluated to optimize a target criteria. For this model analysis, the model tuning will minimize Root Mean Squared Error (RMSE) which is a frequently used measure of the differences between values predicted by the model and the values actually observed.  
  
After the model tuning process, a summary set of model performance criteria will be captured and a performance comparison will be conducted to identify the top performing models based on the training dataset. The performance criteria used in the comparison will include the optimized RMSE value, but also R-square and Mean Absolute Error (MAE). R-squared is a statistical measure of how close the data are to the fitted regression line, and MAE is a measure of the difference between two continuous variables, such as those variables used in the data set.
  

```{r,warning=FALSE}
# required data format, elastic net requires dummy vars for Brand.Code,

Y_train <- trainnew$PH
X_train <- trainnew[, -c(1,26)]
dummies <- dummy.code(trainnew$Brand.Code)
X_train <- cbind(X_train, dummies[, 2:4])

# First, we'll train a set of linear models 
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# LM
set.seed(624)
fit.lm <- train(X_train, Y_train, method="lm", metric=metric,
                preProc=c("center", "scale"), trControl=trainControl)

# GLM
set.seed(624)
fit.glm <- train(X_train, Y_train, method="glm", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# GLMNET
set.seed(624)
fit.glmnet <- train(X_train, Y_train, method="glmnet", metric=metric,
                    preProc=c("center", "scale"), trControl=trainControl)

# Next we'll train the non-linear models 

# SVM
set.seed(624)
fit.svm <- train(X_train, Y_train, method="svmRadial", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# CART
set.seed(624)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(X_train, Y_train, method="rpart", metric=metric,
                  tuneGrid=grid, preProc=c("center", "scale"),
                  trControl=trainControl)
# KNN
set.seed(624)
fit.knn <- train(X_train, Y_train, method="knn", metric=metric,
                 preProc=c("center", "scale"), trControl=trainControl)

# Elastic Net

# fit a grid and center, scale the data
# this model computes quickly
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
                        .fraction = seq(.05, 1, length = 20))

set.seed(624)
fit.nnet <- train(as.matrix(X_train), y_train,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = trainControl,
                  preProc = c("center", "scale"))

# Next we'll train the tree-based models 

# Random Forest
# here we can use Brand.Code as a factor
set.seed(624)
#fit.rf <- train(X_train, Y_train, method="rf", metric=metric,
#                preProc=c("center", "scale"),
#                trControl=trainControl)

# Stochastic Gradient Boosting
set.seed(624)
fit.gbm <- train(X_train, Y_train, method="gbm", metric=metric,
                 preProc=c("center", "scale"),
                 trControl=trainControl, verbose=FALSE)

# Cubist
set.seed(624)
fit.cubist <- train(X_train, Y_train, method="cubist", metric=metric,
                    ppreProc=c("center", "scale"),
                    trControl=trainControl)

# Compare algorithms
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,
                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn, 
                                  NNET=fit.nnet, GBM=fit.gbm, CUBIST=fit.cubist))

# Look at a summary of the results
s <- summary(feature_results)
(s)
```

```{r,warning=FALSE}

# Use the mean value of each feature result category as a basis for model performance comparison

# Here's a comparison plot of the RMSE values
rmse <- s$statistics[["RMSE"]]

dfm <- melt(rmse[,1])

x <- rownames(dfm)

p <- ggplot(dfm,aes(x = reorder(x,value), y = value, fill=x)) + 
    geom_bar(stat = "identity",position = "dodge") +
    xlab(" ") +
  scale_y_continuous("RMSE") + 
  theme(legend.title=element_blank()) +
  labs(title="RMSE Comparison")

(p)

# Here's a comparison plot of the Rsqaured values
rsquared <- s$statistics[["Rsquared"]]

dfm <- melt(rsquared[,1])

x <- rownames(dfm)

p <- ggplot(dfm,aes(x = reorder(x,-value), y = value, fill=x)) + 
    geom_bar(stat = "identity",position = "dodge") +
    xlab(" ") +
  scale_y_continuous("R-Squared") + 
  theme(legend.title=element_blank()) +
  labs(title="R-Squared Comparison")

(p)

# Here's a comparison plot of the MAE values
mae <- s$statistics[["MAE"]]

dfm <- melt(mae[,1])

x <- rownames(dfm)

p <- ggplot(dfm,aes(x = reorder(x,-value), y = value, fill=x)) + 
    geom_bar(stat = "identity",position = "dodge") +
    xlab(" ") +
  scale_y_continuous("MAE") + 
  theme(legend.title=element_blank()) +
  labs(title="MAE Comparison")

(p)
```
  
Next we'll compare the performance of the various models using the Test dataset to generate forecasts, and then analyze the accuracy of the forecasts against the actual values.  
  
```{r,warning=FALSE}
# setup the evaluation data set
Y_test <- testnew$PH
X_test <- testnew[, -c(1,26)]
dummies <- dummy.code(testnew$Brand.Code)
X_test <- cbind(X_test, dummies[, 2:4])

# Start with the linear models again. Generate the forecast, and then analyze the forecast accuracy.

pred <- predict(fit.lm,newdata=X_test)
# generate a summary of how well the forecast values are
acc.lm <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="LM Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.glm,newdata=X_test)
# generate a summary of how well the forecast values are
acc.glm <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="GLM Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.glmnet,newdata=X_test)
# generate a summary of how well the forecast values are
acc.glmnet <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="GLMNET Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

# Perform the same analysis with the non-linear models.
pred <- predict(fit.svm,newdata=X_test)
# generate a summary of how well the forecast values are
acc.svm <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="SVM Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.cart,newdata=X_test)
# generate a summary of how well the forecast values are
acc.cart <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="CART Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.knn,newdata=X_test)
# generate a summary of how well the forecast values are
acc.knn <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="KNN Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.nnet,newdata=X_test)
# generate a summary of how well the forecast values are
acc.nnet <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="NNET Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

# Perform the same analysis with the tree-based models.
pred <- predict(fit.gbm,newdata=X_test)
# generate a summary of how well the forecast values are
acc.gbm <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="GBM Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

pred <- predict(fit.cubist,newdata=X_test)
# generate a summary of how well the forecast values are
acc.cubist <- accuracy(pred,Y_test)
# plot the forecast and the real values
x <- seq(1:nrow(testnew))
# the actual value are in green
plot(x=x, y=Y_test,type="l",col="green", main="CUBIST Forecast Accuracy", ylab="", xlab="")
# the predictions are in red
lines(x=x, y=pred, col="red")

# Here's a comparison plot of the RMSE values
rmse.acc <- vector()
rmse.acc <- c(rmse.acc, acc.lm[2])
rmse.acc <- c(rmse.acc, acc.glm[2])
rmse.acc <- c(rmse.acc, acc.glmnet[2])
rmse.acc <- c(rmse.acc, acc.svm[2])
rmse.acc <- c(rmse.acc, acc.cart[2])
rmse.acc <- c(rmse.acc, acc.knn[2])
rmse.acc <- c(rmse.acc, acc.nnet[2])
rmse.acc <- c(rmse.acc, acc.gbm[2])
rmse.acc <- c(rmse.acc, acc.cubist[2])

rmse.acc.df <- data.frame(rmse.acc,row.names=rownames(dfm))

x <- rownames(dfm)

p <- ggplot(rmse.acc.df,aes(x = reorder(x,rmse.acc), y = rmse.acc, fill=x)) + 
    geom_bar(stat = "identity",position = "dodge") +
    xlab(" ") +
  scale_y_continuous("RMSE") + 
  theme(legend.title=element_blank()) +
  labs(title="RMSE Comparison")

(p)

# Here's a comparison plot of the MAE values
mae.acc <- vector()
mae.acc <- c(mae.acc, acc.lm[3])
mae.acc <- c(mae.acc, acc.glm[3])
mae.acc <- c(mae.acc, acc.glmnet[3])
mae.acc <- c(mae.acc, acc.svm[3])
mae.acc <- c(mae.acc, acc.cart[3])
mae.acc <- c(mae.acc, acc.knn[3])
mae.acc <- c(mae.acc, acc.nnet[3])
mae.acc <- c(mae.acc, acc.gbm[3])
mae.acc <- c(mae.acc, acc.cubist[3])

mae.acc.df <- data.frame(mae.acc,row.names=rownames(dfm))

p <- ggplot(mae.acc.df,aes(x = reorder(x,mae.acc), y = mae.acc, fill=x)) + 
    geom_bar(stat = "identity",position = "dodge") +
    xlab(" ") +
  scale_y_continuous("MAE") + 
  theme(legend.title=element_blank()) +
  labs(title="MAE Comparison")

(p)
```
  
The top performing models are .....  
  
We'll continue with a more in-depth analysis in the next section using these models.  
  
  
# Prediction and Evaluation
-------------------------
- Run tuned models on test Data
- Comparative results chart
- Select best model
- Output predictions to .csv file


