{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Tree-based Models\"\nauthor: \"Tom Detzel\"\ndate: \"11/02/2017\"\noutput: word_document\nsubtitle: CUNY MSDA DATA 624 Homework KJ Ch. 8\n---\n\n```{r setup, include=FALSE, echo=F, message=F, warning=F}\nknitr::opts_chunk$set(echo = TRUE, fig.height=7, fig.width =8)\n\n# load required packages\nlibrary(easypackages)\nlibraries(\"AppliedPredictiveModeling\", \"nnet\", \"kernlab\", \"caret\", \"earth\", \"pander\", \"randomForest\", \"mlbench\",\"ModelMetrics\", \"lars\", \"MASS\", \"stats\", \"pls\", \"psych\", \"corrgram\", \"Hmisc\", \"mi\", \"rpart\", \"party\", \"partykit\", \"gbm\", \"ipred\")\n\n```\n\n\n**8.1. Recreate the simulated data from Exercise 7.2:**\n\n**A.** Code follows.  \n  \n```{r}\nset.seed(200)\nsimulated <- mlbench.friedman1(200, sd = 1)\nsimulated <- cbind(simulated$x, simulated$y)\nsimulated <- as.data.frame(simulated)\ncolnames(simulated)[ncol(simulated)] <- \"y\"\n\n```\n\n**(a) Fit a random forest model to all of the predictors, then estimate the variable importance scores:**\n\n**A.** Code follows.  \n  \n```{r}\nmodel1 <- randomForest(y ~., data = simulated,\n                       importance = TRUE,\n                       ntree = 1000)\n\nrfImp1 <- varImp(model1, scale = FALSE) \n\n```\n\n**Q. Did the random forest model significantly use the uninformative predictors (V6 – V10)?**\n\n**A.** The plots below show that V6-V10 were at the bottom of the perfomance profile.  \n  \n  \n```{r fig.width=5, fig.height=5, fig.align='center'}\nvarImpPlot((model1))\n```\n\n**(b) Now add an additional predictor that is highly correlated with one of the informative predictors. For example: **\n  \n  \n```{r}\n# create correlated variable\nsimulated$duplicate1 <- simulated$V1 + rnorm(200) * .1\ncor(simulated$duplicate1, simulated$V1)\n\n# fit another forest\nmodel2 <- randomForest(y ~., data = simulated,\n                       importance = TRUE,\n                       ntree = 1000)\n\nrfImp2 <- varImp(model2, scale = FALSE) \nrfImp2\n\n```\n  \n  \n**(c) Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?**\n  \n**A.**The second variable dilutes the importance of the highly correlated predictor. In this case the importance score drops from 8.66 to 5.27 for V1.\n  \n**(d) Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?**  \n  \n**A.** The table below normalizes importance measures (percent of total importance) for the Strobl (conditional) statistic and the traditional (unconditional) statistic. As expected, the importance of the highly correlated variable duplicate1 is moderated by the conditional/Strobl approach. It accounts for 16.69 percent of the total importance versus 25.51 percent for unconditional importance. In the table, row 1 has the Strobl values; row 2 has the unconditional values.\n  \nStrobl showed that uninformative predictors with high correlations to informative predictors had abnormally large importance values. As in this case, the duplicate1 variable was more important than V1 in the unconditional importance. The two variables have a correlation of 0.94. \n  \n```{r eval=F}\n# build the new model\ncondFst1 <- cforest(y ~., data = simulated)\n\nvarimp(condFst1)\n\n# compute Strobl and unconditional importance\nconditional <- varimp(condFst1, conditional=T)\nunconditional <- varimp(condFst1, conditional=F)\n\n# normalize importance and compare Strobel (conditional) vs. unconditional contribution\nimp_df <- data.frame(round(rbind((conditional/sum(conditional))*100,\n                                 (unconditional/sum(unconditional))*100),2))\n\nrownames(imp_df) <- c(\"Strobl\", \"Unconditional\")\n\npander(t(imp_df), caption=\"Normalized Variable Importance\")\n\n```\n  \n**(d) Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?**\n\n**A.** With a boosted tree in the gbm package, the function relative.influence measures variable importance. An option allows the result to be scaled; the pattern is similar, showing roughly equivalent importance to V1 and the highly correlated variable duplicate1. There doesn't appear to be an option for Strobl adjustment.\n\n```{r}\n\n# fit a boosted tree\ngbmModel <- gbm(y ~., data = simulated, distribution = \"gaussian\")\n\nrelative.influence(gbmModel)\nrelative.influence(gbmModel, scale.=T)\n\nvarImp(gbmModel, numTrees = 100)\n\n```\n\n**A. cont'd.** A Cubist model. It's fascinating. V2 -- the No. 2 variable in prior models -- is vaulted to the top. V1 is second and the correlated variable, duplicate1, is relegated spot No. 6, knocked off the red carpet. The suggestion -- to be tested in other scenarios before generalizing -- is that Cubist handles correlated variables in a much more refined fashion than basic trees and forests.\n\n```{r}\n\ncubistTuned <- train(simulated[-11], simulated[,11], method = \"cubist\")\n\nvarImp(cubistTuned)\n```\n\n***  \n  \n**8.2 Use a simulation to show tree bias with different granularities.**\n\n**A.** A problem with trees is that they can be overtaken by noise variables. This introduces selection bias. As the text states: \"[P]redictors with a higher number of distinct values are favored over more granular predictors (Loh and Shih 1997; Carolin et al. 2007; Loh 2010).\n\n> “The danger occurs when a data set consists of a mix of informative and noise variables, and the noise variables have many more splits than the informative variables. Then there is a high probability that the noise variables will be chosen to split the top nodes of the tree. Pruning will produce either a tree with misleading structure or no tree at all.” \n\nBelow we create a dataset with four predictors of different granularities and relationships to the response. Two of the variables, X1 and X4, are noise variables -- distributions inconsistent to the response. The model selects those two variables as most important and repeatedly splits on them. \n\n```{r}\n\nset.seed(200)\ny <- rep(1:25, 20)\nX1 <- rnorm(500,1,1)\nX2 <- rep(1:10, 50)\nX3 <- rep(15:24, 50)\nX4 <- rweibull(500, 2, 1)\n\nsimdat <- data.frame(X1, X2, X3, X4, y)\n\n# get train, test sets\ntrain <- sample(1:400, replace=F)\nsim_train <- simdat[train, ]\nsim_test <- simdat[-train, ]\n\n# rpart is basic CART model; ctree is conditional tree\nrpartTree <- rpart(y ~., data = sim_train)\n\n# convert to a plottable object with party\nrpartMyTree <- as.party(rpartTree)\n\n# most important variables\nvarImp(rpartTree)\n```\n\nThe tree plot.\n\n```{r}\n\nplot(rpartMyTree)\n\n```\n\n***\n\n**8.3. In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:\n\n(a) Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?\n\nIn stochastic gradient boosting, the *bagging fraction* is a parameter that refers to the sample size of randomly chosen training data used while iterating through the learning algorithm (in this case, a tree). The parameter ranges from 0.01 to 0.1. Using bagging reduces prediction variance. The *learning rate* is a paramater for regularlizing gradient boosting to prevent overfitting. The rate $lambda$ is between 0 and 1 and refers to the fraction of predicted value that is added after each iteration of the tree algorithm. As the sample size (bagging fraction) and learning rate increase, more of the training data and predicition value is added to the model, which has the effect of concentrating on fewer predictors. Using a $lambda$ of 0.9 means we are using near all the sample data at each iteration, so the model will be greedy and will tend to overfit. Smaller values of $lambda$ mean a greater number of smaller samples are fit, so more variables are likely to be used in the final model. This is more computationally intensive but results in greater accuracy.\n\n\n**(b) Which model do you think would be more predictive of other samples?**  \n  \n**A.**Lower values of $lambda$ and the learning algorithm capture more of the variance in the data and spread variable importance more equitably. They are more computationally intensive but more accurate. So the model on the left will do better.  \n  \n**(c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?**  \n  \n**A.** Because models are being trained with additional interaction terms (i.e., optimal splits), you would expect to see more variables contribute to error improvement. So more variables would be included in the chart, making the curve less of a hockey stick.  \n\n***  \n\n**8.7. Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:**\n\n**(a) Which tree-based regression model gives the optimal resampling and test set performance?**  \n  \n**A.** See results in the table below. A tuned bagged tree produce the lowest RMSE on the test set and on the resampled training data. Models, code and RMSE computations follow.\n\n```{r}\nmodel <- c(\"CART\", \"Bagged Tree\", \"Random Forest\", \"Boosted Tree\")\nsample_err <- c(1.36, 1.32, 1.33, 1.24) \ntest_err <- c(1.6, 1.18, 1.2, 1.11)\n\ntbl_2 <- data.frame(cbind(model, sample_err, test_err))\ncolnames(tbl_2) <- c(\"Model\", \"Sampled RMSE\", \"Test RMSE\")\npander(tbl_2, caption=\"Model Comparison\")\n\n```\n\n**Step 1 - Get the data**  \n\nSince we're working with trees, we won't worry about mutlticollinearity or zero-variance predictors, So we don't need the preprocessing that's required for our other models.\n  \n```{r}\n\n# get the data\ndata(ChemicalManufacturingProcess)\n\n#make it a data frame\ndf <- ChemicalManufacturingProcess\n\n# impute using median using impute() from Hmisc package\ndf <- impute(df, fun=median)\n\n# pull out response\ny = df$Yield\ndf <- df[-1]\n\n# set cross validation var\nctrl <- trainControl(method = \"cv\", number = 10)\n\n#Subset the data into objects for training using integer sub-setting.\nset.seed(200)\ntrainingRows <- createDataPartition(y, p = .70, list = FALSE)\n\nX_train <- df[trainingRows, ]\nX_test <- df[-trainingRows, ]\ny_train <- y[trainingRows]\ny_test <- y[-trainingRows]\n\n```\n\n**Step 2 -- Train several models**\n\n### CART  \n\nWe tuned a CART at length 10 using 10-fold cross validation. The best model had a maxdepth of 4 levels and RMSE of 1.36. See plot and model summary. Error on the test data was 1.6.\n\n```{r}\n# CART (Classification and Regression Tree)\n# set cross validation var\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# rpart example with tuning parameters\nset.seed(100)\nrpartTune <- train(X_train, y_train,\n                   method = \"rpart2\",\n                   tuneLength = 10,\n                   trControl = ctrl)\n\nrpartTune\n\n```\n\n```{r}\n\nplot(rpartTune)\n\n```\n\n\n\n```{r}\n\nrpartPred <- predict(rpartTune, X_test)\ncart_error <- rmse(y_test, rpartPred)\ncart_error\n\n```\n\n### Bagged Tree\n\nA bagged tree using 25 bootstrap replications produced an out-of-bag error estimate of 1.32. The bagged tree produces an appreciably lower RMSE on the test data than CART at 1.18.\n\n```{r message=F, warning=F}\n\nset.seed(100)\nbaggedTree <- bagging(y_train~., data=X_train, coob=T,\n                      nbagg=25, control=\n                 rpart.control(minsplit=2, cp=0, xval=0))\n\n##or\n# baggedTree <- bagging(y ~., data = trainData)\n\nprint(baggedTree)\n```\n\nVariable importance, bagged tree.  \n\n```{r}\nvarImp(baggedTree)\n```\n\nTest data prediction.\n\n```{r}\n# predict on test data\nbagPred <- predict(baggedTree, X_test)\nrmse(y_test, bagPred)\n\n```\n\n### Random Forest\n  \nA random forest of 5,000 trees with the number of predictors randomly chosen at each split set to mtry=25. The test set error was comparable to bagging at 1.2. \n\n```{r}\nset.seed(100)\nrfModel <- randomForest(y_train ~., data = X_train,\n                        mtry=25,\n                        importance=TRUE,\n                        ntree=5000)\n\nrfModel\n\n```\n\n10 most-important variables. Manufacturing process 32 is hugely important in this model.\n\n```{r}\nvarImpPlot(rfModel, n.var=10, type=1)\n```\n\nTest set error for Random Forest:\n\n```{r}\n\n# predict on test data\nrfPred <- predict(rfModel, X_test)\nrmse(y_test, rfPred)\n\n```\n\n### Gradient Boosting\n  \nBoosted trees allow for more tweaking. We built models over a tuning grid of between 100 and 1000 trees, a range of learning rates from 0.01 to 0.1, and a bagging fraction of 0.5. RMSE on the training data was 1.24. On the test data, the model posted RMSE = 1.13, best of the group.\n\n```{r echo=F, warning=F, message=F}\n\ngbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),\n                       .n.trees = seq(100, 1000, by = 50),\n                       .shrinkage = c(0.01, 0.1),\n                       .n.minobsinnode = 10)\n\nset.seed(100)\ngbmTune <- train(X_train, y_train,\n                 method = \"gbm\",\n                 tuneGrid = gbmGrid,\n                 bag.fraction = 0.5,\n                 verbose = FALSE)\n```\n\nPlot of Booosted Tree\n\n```{r}\n\nplot(gbmTune)\n\n```\n  \nModel result\n\n```{r}\n\ngbmTune\n\n```\n\nPredictions on test set\n\n```{r}\n\n# predict on test data\ngbmPred <- predict(gbmTune, X_test)\n\n# very low rmse on test data\nrmse(y_test, gbmPred)\n\n```\n\n**(b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?**  \n  \n**A. ** See list below. Manufacturing Process 32 is by far the most influential. Manufacturing processes are 6 of the top 10 predictors, although two biological processes are second- and third-most important, respectively. By comparison, the optimal nonlinear model, MARS, also chose MF Prodess 32 as most influential and includes several of the same variables in the top 10. The best linear model, Elastic Net, selected MF Process 44 as most important and MF Process 32 as second-most important. It did choose any biological predictors. (Note: These variable importance values are reported in prior excercise submissions.)  \n  \n```{r}\nvarImp(gbmTune)\n```\n\n**(c) Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?**  \n  \n**A.** The plot is instructive because of its easy interpretability. It shows that higher yields are associated with values of MF Process 32 >= to 159.5, values of Bio Material 3 >= to 66.605, and respective right-split values of MF Process 17 and Bio Material 11. Elegant in its simplicy, the tree plot gives clear insight into the interactions of variables that aren't so evident in linear or nonparametric models, which can seem obscure. \n\n\n```{r}\n\nctrl <- rpart.control(maxdepth=2)\nrTree <- rpart(y_train~., X_train, control=ctrl)\n\nplot(as.party(rTree))\n\n```\n\n\n",
    "created" : 1510523429686.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2317178971",
    "id" : "82ECBAA1",
    "lastKnownWriteTime" : 1510341470,
    "last_content_update" : 1510341470,
    "path" : "~/Dropbox/CUNY/IS624/Ch8_Exercises_Detzel.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}