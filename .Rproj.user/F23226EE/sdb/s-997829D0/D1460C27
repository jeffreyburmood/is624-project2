{
    "collab_server" : "",
    "contents" : "---\ntitle: \"\"\nauthor: \"\"\nsubtitle: CUNY MSDA DATA 624\noutput:\n    prettydoc::html_pretty:\n    theme: leonid\n    highlight: github\n    toc: yes\n---\n  \n***  \n<br>\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n#### Get the data\n\nAlso check for missing data\n\n```{r warning=F, message=F}\n\n# load packages\n\n# load required packages\nlibrary(easypackages)\nlibraries(\"AppliedPredictiveModeling\", \"nnet\", \"kernlab\", \"caret\", \"earth\", \"pander\", \"randomForest\", \"mlbench\",\"ModelMetrics\", \"lars\", \"MASS\", \"stats\", \"pls\", \"psych\", \"corrplot\", \"Hmisc\", \"mi\", \"rpart\", \"party\", \"partykit\", \"gbm\", \"ipred\")\n\n# read in the data\ndf <- read.csv(\"data/StudentData.csv\", header=T, strip.white=T)\nstr(df)\n\n# is 0 a stand-in for NA?\nzero_vals <- data.frame(cbind(colSums(df==0)))\ncolnames(zero_vals) <- \"zero_count\"\nwrite.csv(zero_vals, \"data/zero_vals.csv\")\n\ntable(df$Hyd.Pressure1)\n\n\n\n```\n\n***  \n\n<br> \n\n#### Make a data partition  \n  \n```{r}\n\n# Split out validation dataset\n# create an 80/20 train/test set\nset.seed(100)\nvalidationIndex <- createDataPartition(BostonHousing$medv, p=0.80, list=FALSE)\n\n# validation\nvalidation <- BostonHousing[-validationIndex,]\n\n# training and testing\ndataset <- BostonHousing[validationIndex,]\n\n# turn chas variable into numeric to make plotting easier\ndataset[,4] <- as.numeric(as.character(dataset[,4]))\n\n```\n***  \n<br>\n\n#### Check for multicollinearity  \n\nSeveral predictors are correlated, which might reduce accuracy.  \n  \n```{r}\n\n# cor(dataset[,1:13])\n\n# which ones are too above cor .70?\ntooHigh <- findCorrelation(cor(dataset), cutoff = .70)\nhead(dataset[, tooHigh])\n\n```\n***  \n<br>\n\n#### Plot variable distributions\n\n```{r eval=F, fig.width=11}\n# histograms each attribute\npar(mfrow=c(2,7))\nfor(i in 1:13) {\n  hist(dataset[,i], main=names(dataset)[i])\n}\n```\n***  \n<br>\n\n#### Try it with density curves\n\n```{r eval=F, fig.width=11}\n# density plot for each attribute\npar(mfrow=c(2,7))\nfor(i in 1:13) {\n  plot(density(dataset[,i]), main=names(dataset)[i])\n}\n```\n***  \n<br>\n\n#### Box and whisker plots\n\n```{r, fig.width=12}\n# boxplots for each attribute\npar(mfrow=c(2,7))\nfor(i in 1:13) {\n  boxplot(dataset[,i], main=names(dataset)[i])\n}\n```\n***  \n<br>\n\n#### Corplot to look for multicollinearity\n\nRed and Blue circles identify suspect variables for dropping.\n\n```{r}\n# correlation plot\ncorrelations <- cor(dataset[,1:13])\ncorrplot(correlations, method=\"circle\")\n```\n***  \n<br>\n\n#### Set up 10-fold cross-validation  \n\n**Caret helpers:** trainControl(), RMSE(), expand.grid(), resamples()\n\n```{r}\n\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n```\n***  \n<br>\n\n#### Fit baseline models  \n  \n**Models:** OLS, GLM, Elastic Net, SVM, CART, KNN  \n  \n**Caret:** Use helpers trainControl(), RMSE(), expand.grid(), resamples()  \n  \n```{r}\n\n# LM\n# a straight-ahead linear model, no frills except for centering and scaling data\nset.seed(100)\nfit.lm <- train(medv~., data=dataset, method=\"lm\", metric=metric, \n                preProc=c(\"center\", \"scale\"), trControl=trainControl)\n# GLM\nset.seed(100)\nfit.glm <- train(medv~., data=dataset, method=\"glm\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n# GLMNET, similar to lasso\nset.seed(100)\nfit.glmnet <- train(medv~., data=dataset, method=\"glmnet\", metric=metric,\n                    preProc=c(\"center\", \"scale\"), trControl=trainControl)\n# SVM\nset.seed(100)\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n\n# CART, using a grid of penalty values\nset.seed(100)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=dataset, method=\"rpart\", metric=metric,\n                  tuneGrid=grid, preProc=c(\"center\", \"scale\"),\n                  trControl=trainControl)\n# KNN\nset.seed(100)\nfit.knn <- train(medv~., data=dataset, method=\"knn\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n```\n***  \n<br>\n\n#### Amazing compare results table from Caret\n\n```{r}\n# Compare algorithms\nresults <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm,\n    CART=fit.cart, KNN=fit.knn))\nsummary(results)\ndotplot(results)\n\n```\n***  \n\n<br>\n\n#### Multicollinearity? \n\nPerformance is poor. We can try removing collinear variables.\n  \n```{r}\n# remove correlated attributes\n# find attributes that are highly correlated\nset.seed(100)\ncutoff <- 0.70\ncorrelations <- cor(dataset[,1:13])\nhighlyCorrelated <- findCorrelation(correlations, cutoff=cutoff)\nfor (value in highlyCorrelated) {\n  print(names(dataset)[value])\n}\n\n# create a new dataset without highly correlated features\ndatasetFeatures <- dataset[,-highlyCorrelated]\ndim(datasetFeatures)\n\n```\n***  \n<br> \n\n#### Refit our six models once again\n\n**Caret:** Use helper functions trainControl(), RMSE(), expand.grid(), resamples()  \n  \n```{r}\n\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# LM\nset.seed(100)\nfit.lm <- train(medv~., data=datasetFeatures, method=\"lm\", metric=metric,\n                preProc=c(\"center\", \"scale\"), trControl=trainControl)\n\n# GLM\nset.seed(100)\nfit.glm <- train(medv~., data=datasetFeatures, method=\"glm\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n# GLMNET\nset.seed(100)\nfit.glmnet <- train(medv~., data=datasetFeatures, method=\"glmnet\", metric=metric,\n                    preProc=c(\"center\", \"scale\"), trControl=trainControl)\n# SVM\nset.seed(100)\nfit.svm <- train(medv~., data=datasetFeatures, method=\"svmRadial\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n\n# CART\nset.seed(100)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=datasetFeatures, method=\"rpart\", metric=metric,\n                  tuneGrid=grid, preProc=c(\"center\", \"scale\"),\n                  trControl=trainControl)\n# KNN\nset.seed(100)\nfit.knn <- train(medv~., data=datasetFeatures, method=\"knn\", metric=metric,\n                 preProc=c(\"center\", \"scale\"), trControl=trainControl)\n\n# Compare algorithms\nfeature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet,\n                                  SVM=fit.svm, CART=fit.cart, KNN=fit.knn))\n\nsummary(feature_results)\n\n```\n***  \n<br>  \n\n### Plot a comparison  \n  \n```{r}\n\ndotplot(feature_results)\n\n```\n***  \n<br>\n\n#### Removing vars didn't help; try a transformation  \n  \nRMSE is worse, so the correlated variables have value.  \n  \nA **Box-Cox power transform** rescales the data. This is a **preprocessing parameter.**  \n  \n```{r}\n\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# LM\nset.seed(100)\nfit.lm <- train(medv~., data=dataset, method=\"lm\", metric=metric, preProc=c(\"center\",\n    \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# GLM\nset.seed(100)\nfit.glm <- train(medv~., data=dataset, method=\"glm\", metric=metric, preProc=c(\"center\",\n    \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# GLMNET\nset.seed(100)\nfit.glmnet <- train(medv~., data=dataset, method=\"glmnet\", metric=metric,\n    preProc=c(\"center\", \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# SVM\nset.seed(100)\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", metric=metric,\n    preProc=c(\"center\", \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# CART\nset.seed(100)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=dataset, method=\"rpart\", metric=metric, tuneGrid=grid,\n    preProc=c(\"center\", \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# KNN\nset.seed(100)\nfit.knn <- train(medv~., data=dataset, method=\"knn\", metric=metric, preProc=c(\"center\",\n    \"scale\", \"BoxCox\"), trControl=trainControl)\n\n# Compare algorithms\ntransformResults <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm,\n    CART=fit.cart, KNN=fit.knn))\nsummary(transformResults)\n\n```\n***  \n\n<br>  \n\n#### Let's try ensemble methods  \n    \n```{r}\n# try ensembles\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# Random Forest\nset.seed(100)\n\nfit.rf <- train(medv~., data=dataset, method=\"rf\", metric=metric,\n                preProc=c(\"BoxCox\"),\n                trControl=trainControl)\n\n# Stochastic Gradient Boosting\nset.seed(100)\nfit.gbm <- train(medv~., data=dataset, method=\"gbm\", metric=metric,\n                 preProc=c(\"BoxCox\"),\n                 trControl=trainControl, verbose=FALSE)\n\n# Cubist\nset.seed(100)\nfit.cubist <- train(medv~., data=dataset, method=\"cubist\", metric=metric,\n                    preProc=c(\"BoxCox\"), trControl=trainControl)\n\n# Compare algorithms\nensembleResults <- resamples(list(RF=fit.rf, GBM=fit.gbm, CUBIST=fit.cubist))\n\nsummary(ensembleResults)\n\n```\n***  \n\n<br>  \n  \n```{r}\n\ndotplot(ensembleResults)\n\n```\n***  \n\n<br> \n\n#### Look at Cubist more closely  \n  \nIt had the lowest resampling error.  \n  \n```{r}\n# look at parameters used for Cubist\nprint(fit.cubist)\n\n```\n***  \n\n<br>  \n\n#### What are the Cubist tuning parameters?\n\n**Committees:** The number of boosting iterations in the model. Each 'committee' has multiple rules that are used to predict new samples, which are averaged. \n  \n**Neighbors:** When the rule-based model is finished, Cubist can adjust it with a k-neighbors process that weights each result. Can tune over this parmeter, say 3-7. \n  \n**Grid search:** We will tune with a grid search over 10 committees and three neighbor values using 10-fold cross validation repeated three times and RMSE as our metric.  \n  \n```{r}\n\n# Tune the Cubist algorithm\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\nset.seed(100)\n\ngrid <- expand.grid(.committees=seq(15, 25, by=1), .neighbors=c(3, 5, 7))\n\ntune.cubist <- train(medv~., data=dataset, method=\"cubist\", metric=metric,\n    preProc=c(\"BoxCox\"), tuneGrid=grid, trControl=trainControl)\n\nprint(tune.cubist)\n\n```\n***  \n\n<br>\n\n#### Plot the cubist solution  \n  \n```{r}\n\nplot(tune.cubist)\n\n```\n***  \n\n<br> \n\n#### Finalize the Cubist model\n\n```{r}\n# prepare the data transform using training data\nset.seed(100)\nx <- dataset[,1:13]\ny <- dataset[,14]\npreprocessParams <- preProcess(x, method=c(\"BoxCox\"))\ntransX <- predict(preprocessParams, x)\n\n# train the final model\nfinalModel <- cubist(x=transX, y=y, committees=18)\n\nfinalModel\n```\n***  \n\n<br> \n\n#### Predict from the final model \n\n```{r}\n\n# transform the validation dataset\nset.seed(100)\nvalX <- validation[,1:13]\ntrans_valX <- predict(preprocessParams, valX)\nvalY <- validation[,14]\n\n# use final model to make predictions on the validation dataset\npredictions <- predict(finalModel, newdata=trans_valX, neighbors=3)\n\n# calculate RMSE\nrmse <- RMSE(predictions, valY)\nr2 <- R2(predictions, valY)\n\nprint(rmse)\n\n```\n\n",
    "created" : 1510523274871.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3590645188",
    "id" : "D1460C27",
    "lastKnownWriteTime" : 1510529682,
    "last_content_update" : 1510529682957,
    "path" : "~/Dropbox/CUNY/IS624/is624-project2/code/explore_PH.Rmd",
    "project_path" : "code/explore_PH.Rmd",
    "properties" : {
        "last_setup_crc32" : ""
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}